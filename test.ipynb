{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a3b35e-28cd-42f2-a518-1116803df2ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Python code to retrieve HTML content from URL and extract product specs from the URL\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# Step 1: Define function to fetch and clean text from Product Spec URL\n",
    "def fetch_and_clean_text(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status() #Raises HTTP Error for Bad Responses\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Failed to fetch URL {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    raw_text = soup.get_text()\n",
    "    cleaned_text = '\\n'.join(\n",
    "        line.strip() for line in raw_text.splitlines() if line.strip()\n",
    "    )\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "# Step 2; Fetch the specs from the URLs\n",
    "#url1 = \"https://www.qualcomm.com/products/mobile/snapdragon/smartphones/snapdragon-8-series-mobile-platforms/snapdragon-8s-gen-4-mobile-platform#features\"\n",
    "#url2 = \"https://www.qualcomm.com/products/mobile/snapdragon/smartphones/snapdragon-6-series-mobile-platforms/snapdragon-6-gen-4-mobile-platform\"\n",
    "\n",
    "url1 = \"https://www.intel.com/content/www/us/en/products/docs/processors/core/core-14th-gen-desktop-brief.html\"\n",
    "url2 = \"https://www.intel.com/content/www/us/en/products/docs/processors/core/core-14th-gen-desktop-brief.html\" \n",
    "# Create alistof URLs and Iterate - for later Step. dummy placeholder code for now\n",
    "product_urls = [url1, url2, ...]\n",
    "\n",
    "\n",
    "product_spec_1 = fetch_and_clean_text(url1)\n",
    "product_spec_2 = fetch_and_clean_text(url2)\n",
    "\n",
    "print(product_spec_1)\n",
    "print(product_spec_2)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b55cc91-eb1b-417c-a170-9c52880fbccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Setup Cell. Getting the OpenAPI Keys...Do I need something similar for other Models & make this a generic wrapper ?\n",
    "\n",
    "#Init block\n",
    "\n",
    "import openai\n",
    "import os\n",
    "\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_= load_dotenv(find_dotenv()) # read local .env file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d993e1ec-e68e-4c04-a1bf-03f743a16b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Supported Models Block\n",
    "    \n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Summarize the moon landing in one sentence.\"}]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c692873-cde0-4d05-ba6f-f8006a99cb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function Definitions Block\n",
    "\n",
    "#This snippet will print all Models that can be accessed. currently using gpt-3.5.-turbo, no access to gpt-4\n",
    "\n",
    "# Initialize OpenAI always..since I might be running this snippet alone often\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "#First attempt at Prompt Parsing ... Provide a Wrapper function get_completion that takes in the Prompt \n",
    "# model variable passed as hardcoded String, should this be a Variable based on the initial Choice ?\n",
    "\n",
    "def get_completion (prompt, model = \"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "         model=model,\n",
    "         messages=messages,\n",
    "         temperature=0, #Randomness degree, important to have the same variale name as temperature \n",
    "    )\n",
    "    \n",
    "    #Return the output of the get_completion, and print it before returning \n",
    "    print(response.choices[0].message.content)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "#call get_completion # test code\n",
    "get_completion(\"this is a test code\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4479de-a51d-47aa-b707-e9febf182fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some real code - basics\n",
    "#Initalize stuff...Run  the init block, client setup and func defintions:  get_completion block before...\n",
    "\n",
    "# Step 1: Simple stuff..compare 2 product specs provided in 2 html links, list out their KPIs and Comparison in a table\n",
    "# Step 2: List out any missing KPIs that should have been considered\n",
    "# Step 3: Loop thru a set of Product spec links, provided as a list and do the same \n",
    "# Step 4: Draw a grap on the evolution of metrics/numbers for each of the KPI, vs year(time)\n",
    "\n",
    "# Initialize the product specifications to 2 variables.  At a later stage, this needs to be populated automatically \n",
    "#by another section of code that would create an array/list with product specification links, automatically from one Vendor\n",
    "#Last step Create a code section to  automate acquistion of various product specs from all list of Vendors viz, Nvidia, QC, Intel, BRoadcom etc..\n",
    "\n",
    "\n",
    "## Prompt activity below...\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Your task is to help the product management team create a comparison of the KPIs of the semiconductor products\n",
    "based on the product specifications provided in the links for each of the semiconductor products.\n",
    "\n",
    "Create a table, with a list of KPIs and their comparison. Provide a SWOT at the end of the comparison.\n",
    "\n",
    "The technical specifications of the products to be compared are delimited by triple backticks\n",
    "\n",
    "First product specification: ```{product_specification_1}```\n",
    "Second product specification: ```{product_specification_2}```\n",
    "\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print (response)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f459b9b3-aebd-45aa-a74f-1f205d2acfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This snippet is for parsing the Financial Reports of Corporates and highlight Key factors, risks and potential\n",
    "# Initialize Setup Cell. Getting the OpenAPI Keys...Do I need something similar for other Models & make this a generic wrapper ?\n",
    "# This is a self contained block of code, including imports, initialization, get_completion function definition etc..\n",
    "\n",
    "#Init block\n",
    "\n",
    "import openai\n",
    "import os\n",
    "\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_= load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "#Function Definitions Block\n",
    "\n",
    "#This snippet will print all Models that can be accessed. currently using gpt-3.5.-turbo, no access to gpt-4\n",
    "\n",
    "# Initialize OpenAI always..since I might be running this snippet alone often\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "#First attempt at Prompt Parsing ... Provide a Wrapper function get_completion that takes in the Prompt \n",
    "# model variable passed as hardcoded String, should this be a Variable based on the initial Choice ?\n",
    "\n",
    "def get_completion (prompt, model = \"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "         model=model,\n",
    "         messages=messages,\n",
    "         temperature=0, #Randomness degree, important to have the same variale name as temperature \n",
    "    )\n",
    "    \n",
    "    #Return the output of the get_completion, and print it before returning \n",
    "    #print(response.choices[0].message.content)\n",
    "    return response.choices[0].message.content\n",
    "    \n",
    "\n",
    "prompt = f\"\"\"\n",
    "Your task is to parse the financial report of the company provided and do the following\n",
    "\n",
    "1. Extract Key financial KPIS:\n",
    "\n",
    "  Overall:\n",
    "  \n",
    " - Revenue\n",
    " - Operating Expenses\n",
    " - Operating  Margin\n",
    " - EBITDA\n",
    " - Net Income\n",
    " - Operating Margin\n",
    "\n",
    " Profitability:\n",
    " \n",
    " -  Gross Profit Margin:\n",
    " -  Net Profit Margin: (Net Income / Revenue) \n",
    " -  Return on Equity (ROE): (Net Income / Shareholders' Equity) \n",
    " -  Return on Assets (ROA): (Net Income / Total Assets)\n",
    "    \n",
    " - YoY Growth %\n",
    " - EPS (Earnings per Share)\n",
    " - FCF (Free Cash Flow)\n",
    " - Debt Levels\n",
    " - \n",
    " \n",
    "Highlight other KPIs as required.\n",
    "If there is anything unusual that might highlight growth, risks and other factors highlight those.\n",
    "The Financial Reports of the company are delimited by triple backticks\n",
    "\n",
    "Company Financial Report: ```{financial_report_1}```\n",
    "\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c24f2c4-92b0-4d90-96bd-750e150b19aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test code to see how to use OpenRouter and what model are supported in Mistral\n",
    "\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "openai.api_base = \"https://openrouter.ai/api/v1\"\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"mistralai/mixtral-8x7b-instruct\",  # or any model available on OpenRouter\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a financial analysis assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Parse this financial report and extract key KPIs...\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response['choices'][0]['message']['content'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4456281f-8118-4336-93b0-618e0dbc779e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Raw response (as text):\n",
      " {\"data\":[{\"id\":\"google/gemini-2.5-pro-preview-03-25\",\"name\":\"Google: Gemini 2.5 Pro Preview\",\"created\":1744924206,\"description\":\"Gemini 2.5 Pro is Google’s state-of-the-art AI model designed for advanced reasoning, coding, mathematics, and scientific tasks. It employs “thinking” capabilities, enabling it to reason through responses with enhanced accuracy and nuanced context handling. Gemini 2.5 Pro achieves top-tier performance on multiple benchmarks, including first-place positioning on the LMA\n",
      "✅ Available Models on OpenRouter:\n",
      "\n",
      "google/gemini-2.5-pro-preview-03-25      | Google: Gemini 2.5 Pro Preview\n",
      "thudm/glm-z1-32b:free                    | THUDM: GLM Z1 32B (free)\n",
      "thudm/glm-4-32b:free                     | THUDM: GLM 4 32B (free)\n",
      "google/gemini-2.5-flash-preview          | Google: Gemini 2.5 Flash Preview\n",
      "google/gemini-2.5-flash-preview:thinking | Google: Gemini 2.5 Flash Preview (thinking)\n",
      "openai/o4-mini-high                      | OpenAI: o4 Mini High\n",
      "openai/o3                                | OpenAI: o3\n",
      "openai/o4-mini                           | OpenAI: o4 Mini\n",
      "shisa-ai/shisa-v2-llama3.3-70b:free      | Shisa AI: Shisa V2 Llama 3.3 70B  (free)\n",
      "qwen/qwen2.5-coder-7b-instruct           | Qwen: Qwen2.5 Coder 7B Instruct\n",
      "openai/gpt-4.1                           | OpenAI: GPT-4.1\n",
      "openai/gpt-4.1-mini                      | OpenAI: GPT-4.1 Mini\n",
      "openai/gpt-4.1-nano                      | OpenAI: GPT-4.1 Nano\n",
      "eleutherai/llemma_7b                     | EleutherAI: Llemma 7b\n",
      "alfredpros/codellama-7b-instruct-solidity | AlfredPros: CodeLLaMa 7B Instruct Solidity\n",
      "arliai/qwq-32b-arliai-rpr-v1:free        | ArliAI: QwQ 32B RpR v1 (free)\n",
      "agentica-org/deepcoder-14b-preview:free  | Agentica: Deepcoder 14B Preview (free)\n",
      "moonshotai/kimi-vl-a3b-thinking:free     | Moonshot AI: Kimi VL A3B Thinking (free)\n",
      "x-ai/grok-3-mini-beta                    | xAI: Grok 3 Mini Beta\n",
      "x-ai/grok-3-beta                         | xAI: Grok 3 Beta\n",
      "nvidia/llama-3.1-nemotron-nano-8b-v1:free | NVIDIA: Llama 3.1 Nemotron Nano 8B v1 (free)\n",
      "nvidia/llama-3.3-nemotron-super-49b-v1:free | NVIDIA: Llama 3.3 Nemotron Super 49B v1 (free)\n",
      "nvidia/llama-3.1-nemotron-ultra-253b-v1:free | NVIDIA: Llama 3.1 Nemotron Ultra 253B v1 (free)\n",
      "meta-llama/llama-4-maverick:free         | Meta: Llama 4 Maverick (free)\n",
      "meta-llama/llama-4-maverick              | Meta: Llama 4 Maverick\n",
      "meta-llama/llama-4-scout:free            | Meta: Llama 4 Scout (free)\n",
      "meta-llama/llama-4-scout                 | Meta: Llama 4 Scout\n",
      "all-hands/openhands-lm-32b-v0.1          | OpenHands LM 32B V0.1\n",
      "mistral/ministral-8b                     | Mistral: Ministral 8B\n",
      "deepseek/deepseek-v3-base:free           | DeepSeek: DeepSeek V3 Base (free)\n",
      "scb10x/llama3.1-typhoon2-8b-instruct     | Typhoon2 8B Instruct\n",
      "scb10x/llama3.1-typhoon2-70b-instruct    | Typhoon2 70B Instruct\n",
      "allenai/molmo-7b-d:free                  | AllenAI: Molmo 7B D (free)\n",
      "bytedance-research/ui-tars-72b:free      | Bytedance: UI-TARS 72B  (free)\n",
      "qwen/qwen2.5-vl-3b-instruct:free         | Qwen: Qwen2.5 VL 3B Instruct (free)\n",
      "google/gemini-2.5-pro-exp-03-25:free     | Google: Gemini 2.5 Pro Experimental (free)\n",
      "qwen/qwen2.5-vl-32b-instruct:free        | Qwen: Qwen2.5 VL 32B Instruct (free)\n",
      "qwen/qwen2.5-vl-32b-instruct             | Qwen: Qwen2.5 VL 32B Instruct\n",
      "deepseek/deepseek-chat-v3-0324:free      | DeepSeek: DeepSeek V3 0324 (free)\n",
      "deepseek/deepseek-chat-v3-0324           | DeepSeek: DeepSeek V3 0324\n",
      "featherless/qwerky-72b:free              | Qwerky 72B (free)\n",
      "openai/o1-pro                            | OpenAI: o1-pro\n",
      "mistralai/mistral-small-3.1-24b-instruct:free | Mistral: Mistral Small 3.1 24B (free)\n",
      "mistralai/mistral-small-3.1-24b-instruct | Mistral: Mistral Small 3.1 24B\n",
      "open-r1/olympiccoder-7b:free             | OlympicCoder 7B (free)\n",
      "open-r1/olympiccoder-32b:free            | OlympicCoder 32B (free)\n",
      "steelskull/l3.3-electra-r1-70b           | SteelSkull: L3.3 Electra R1 70B\n",
      "google/gemma-3-1b-it:free                | Google: Gemma 3 1B (free)\n",
      "google/gemma-3-4b-it:free                | Google: Gemma 3 4B (free)\n",
      "google/gemma-3-4b-it                     | Google: Gemma 3 4B\n",
      "ai21/jamba-1.6-large                     | AI21: Jamba 1.6 Large\n",
      "ai21/jamba-1.6-mini                      | AI21: Jamba Mini 1.6\n",
      "google/gemma-3-12b-it:free               | Google: Gemma 3 12B (free)\n",
      "google/gemma-3-12b-it                    | Google: Gemma 3 12B\n",
      "cohere/command-a                         | Cohere: Command A\n",
      "openai/gpt-4o-mini-search-preview        | OpenAI: GPT-4o-mini Search Preview\n",
      "openai/gpt-4o-search-preview             | OpenAI: GPT-4o Search Preview\n",
      "rekaai/reka-flash-3:free                 | Reka: Flash 3 (free)\n",
      "google/gemma-3-27b-it:free               | Google: Gemma 3 27B (free)\n",
      "google/gemma-3-27b-it                    | Google: Gemma 3 27B\n",
      "thedrummer/anubis-pro-105b-v1            | TheDrummer: Anubis Pro 105B V1\n",
      "latitudegames/wayfarer-large-70b-llama-3.3 | LatitudeGames: Wayfarer Large 70B Llama 3.3\n",
      "thedrummer/skyfall-36b-v2                | TheDrummer: Skyfall 36B V2\n",
      "microsoft/phi-4-multimodal-instruct      | Microsoft: Phi 4 Multimodal Instruct\n",
      "perplexity/sonar-reasoning-pro           | Perplexity: Sonar Reasoning Pro\n",
      "perplexity/sonar-pro                     | Perplexity: Sonar Pro\n",
      "perplexity/sonar-deep-research           | Perplexity: Sonar Deep Research\n",
      "deepseek/deepseek-r1-zero:free           | DeepSeek: DeepSeek R1 Zero (free)\n",
      "qwen/qwq-32b:free                        | Qwen: QwQ 32B (free)\n",
      "qwen/qwq-32b                             | Qwen: QwQ 32B\n",
      "moonshotai/moonlight-16b-a3b-instruct:free | Moonshot AI: Moonlight 16B A3B Instruct (free)\n",
      "nousresearch/deephermes-3-llama-3-8b-preview:free | Nous: DeepHermes 3 Llama 3 8B Preview (free)\n",
      "openai/gpt-4.5-preview                   | OpenAI: GPT-4.5 (Preview)\n",
      "google/gemini-2.0-flash-lite-001         | Google: Gemini 2.0 Flash Lite\n",
      "anthropic/claude-3.7-sonnet              | Anthropic: Claude 3.7 Sonnet\n",
      "anthropic/claude-3.7-sonnet:thinking     | Anthropic: Claude 3.7 Sonnet (thinking)\n",
      "anthropic/claude-3.7-sonnet:beta         | Anthropic: Claude 3.7 Sonnet (self-moderated)\n",
      "perplexity/r1-1776                       | Perplexity: R1 1776\n",
      "mistralai/mistral-saba                   | Mistral: Saba\n",
      "cognitivecomputations/dolphin3.0-r1-mistral-24b:free | Dolphin3.0 R1 Mistral 24B (free)\n",
      "cognitivecomputations/dolphin3.0-mistral-24b:free | Dolphin3.0 Mistral 24B (free)\n",
      "meta-llama/llama-guard-3-8b              | Llama Guard 3 8B\n",
      "openai/o3-mini-high                      | OpenAI: o3 Mini High\n",
      "deepseek/deepseek-r1-distill-llama-8b    | DeepSeek: R1 Distill Llama 8B\n",
      "google/gemini-2.0-flash-001              | Google: Gemini 2.0 Flash\n",
      "qwen/qwen-vl-plus                        | Qwen: Qwen VL Plus\n",
      "aion-labs/aion-1.0                       | AionLabs: Aion-1.0\n",
      "aion-labs/aion-1.0-mini                  | AionLabs: Aion-1.0-Mini\n",
      "aion-labs/aion-rp-llama-3.1-8b           | AionLabs: Aion-RP 1.0 (8B)\n",
      "qwen/qwen-vl-max                         | Qwen: Qwen VL Max\n",
      "qwen/qwen-turbo                          | Qwen: Qwen-Turbo\n",
      "qwen/qwen2.5-vl-72b-instruct:free        | Qwen: Qwen2.5 VL 72B Instruct (free)\n",
      "qwen/qwen2.5-vl-72b-instruct             | Qwen: Qwen2.5 VL 72B Instruct\n",
      "qwen/qwen-plus                           | Qwen: Qwen-Plus\n",
      "qwen/qwen-max                            | Qwen: Qwen-Max \n",
      "openai/o3-mini                           | OpenAI: o3 Mini\n",
      "deepseek/deepseek-r1-distill-qwen-1.5b   | DeepSeek: R1 Distill Qwen 1.5B\n",
      "mistralai/mistral-small-24b-instruct-2501:free | Mistral: Mistral Small 3 (free)\n",
      "mistralai/mistral-small-24b-instruct-2501 | Mistral: Mistral Small 3\n",
      "deepseek/deepseek-r1-distill-qwen-32b:free | DeepSeek: R1 Distill Qwen 32B (free)\n",
      "deepseek/deepseek-r1-distill-qwen-32b    | DeepSeek: R1 Distill Qwen 32B\n",
      "deepseek/deepseek-r1-distill-qwen-14b:free | DeepSeek: R1 Distill Qwen 14B (free)\n",
      "deepseek/deepseek-r1-distill-qwen-14b    | DeepSeek: R1 Distill Qwen 14B\n",
      "perplexity/sonar-reasoning               | Perplexity: Sonar Reasoning\n",
      "perplexity/sonar                         | Perplexity: Sonar\n",
      "liquid/lfm-7b                            | Liquid: LFM 7B\n",
      "liquid/lfm-3b                            | Liquid: LFM 3B\n",
      "deepseek/deepseek-r1-distill-llama-70b:free | DeepSeek: R1 Distill Llama 70B (free)\n",
      "deepseek/deepseek-r1-distill-llama-70b   | DeepSeek: R1 Distill Llama 70B\n",
      "google/gemini-2.0-flash-thinking-exp:free | Google: Gemini 2.0 Flash Thinking Experimental 01-21 (free)\n",
      "deepseek/deepseek-r1:free                | DeepSeek: R1 (free)\n",
      "deepseek/deepseek-r1                     | DeepSeek: R1\n",
      "sophosympatheia/rogue-rose-103b-v0.2:free | Rogue Rose 103B v0.2 (free)\n",
      "minimax/minimax-01                       | MiniMax: MiniMax-01\n",
      "mistralai/codestral-2501                 | Mistral: Codestral 2501\n",
      "microsoft/phi-4                          | Microsoft: Phi 4\n",
      "deepseek/deepseek-chat:free              | DeepSeek: DeepSeek V3 (free)\n",
      "deepseek/deepseek-chat                   | DeepSeek: DeepSeek V3\n",
      "google/gemini-2.0-flash-thinking-exp-1219:free | Google: Gemini 2.0 Flash Thinking Experimental (free)\n",
      "sao10k/l3.3-euryale-70b                  | Sao10K: Llama 3.3 Euryale 70B\n",
      "openai/o1                                | OpenAI: o1\n",
      "eva-unit-01/eva-llama-3.33-70b           | EVA Llama 3.33 70B\n",
      "x-ai/grok-2-vision-1212                  | xAI: Grok 2 Vision 1212\n",
      "x-ai/grok-2-1212                         | xAI: Grok 2 1212\n",
      "cohere/command-r7b-12-2024               | Cohere: Command R7B (12-2024)\n",
      "google/gemini-2.0-flash-exp:free         | Google: Gemini 2.0 Flash Experimental (free)\n",
      "meta-llama/llama-3.3-70b-instruct:free   | Meta: Llama 3.3 70B Instruct (free)\n",
      "meta-llama/llama-3.3-70b-instruct        | Meta: Llama 3.3 70B Instruct\n",
      "amazon/nova-lite-v1                      | Amazon: Nova Lite 1.0\n",
      "amazon/nova-micro-v1                     | Amazon: Nova Micro 1.0\n",
      "amazon/nova-pro-v1                       | Amazon: Nova Pro 1.0\n",
      "qwen/qwq-32b-preview:free                | Qwen: QwQ 32B Preview (free)\n",
      "qwen/qwq-32b-preview                     | Qwen: QwQ 32B Preview\n",
      "google/learnlm-1.5-pro-experimental:free | Google: LearnLM 1.5 Pro Experimental (free)\n",
      "eva-unit-01/eva-qwen-2.5-72b             | EVA Qwen2.5 72B\n",
      "openai/gpt-4o-2024-11-20                 | OpenAI: GPT-4o (2024-11-20)\n",
      "mistralai/mistral-large-2411             | Mistral Large 2411\n",
      "mistralai/mistral-large-2407             | Mistral Large 2407\n",
      "mistralai/pixtral-large-2411             | Mistral: Pixtral Large 2411\n",
      "x-ai/grok-vision-beta                    | xAI: Grok Vision Beta\n",
      "infermatic/mn-inferor-12b                | Infermatic: Mistral Nemo Inferor 12B\n",
      "qwen/qwen-2.5-coder-32b-instruct:free    | Qwen2.5 Coder 32B Instruct (free)\n",
      "qwen/qwen-2.5-coder-32b-instruct         | Qwen2.5 Coder 32B Instruct\n",
      "raifle/sorcererlm-8x22b                  | SorcererLM 8x22B\n",
      "eva-unit-01/eva-qwen-2.5-32b             | EVA Qwen2.5 32B\n",
      "thedrummer/unslopnemo-12b                | Unslopnemo 12B\n",
      "anthropic/claude-3.5-haiku:beta          | Anthropic: Claude 3.5 Haiku (self-moderated)\n",
      "anthropic/claude-3.5-haiku               | Anthropic: Claude 3.5 Haiku\n",
      "anthropic/claude-3.5-haiku-20241022:beta | Anthropic: Claude 3.5 Haiku (2024-10-22) (self-moderated)\n",
      "anthropic/claude-3.5-haiku-20241022      | Anthropic: Claude 3.5 Haiku (2024-10-22)\n",
      "neversleep/llama-3.1-lumimaid-70b        | NeverSleep: Lumimaid v0.2 70B\n",
      "anthropic/claude-3.5-sonnet:beta         | Anthropic: Claude 3.5 Sonnet (self-moderated)\n",
      "anthropic/claude-3.5-sonnet              | Anthropic: Claude 3.5 Sonnet\n",
      "anthracite-org/magnum-v4-72b             | Magnum v4 72B\n",
      "x-ai/grok-beta                           | xAI: Grok Beta\n",
      "mistralai/ministral-8b                   | Mistral: Ministral 8B\n",
      "mistralai/ministral-3b                   | Mistral: Ministral 3B\n",
      "qwen/qwen-2.5-7b-instruct:free           | Qwen2.5 7B Instruct (free)\n",
      "qwen/qwen-2.5-7b-instruct                | Qwen2.5 7B Instruct\n",
      "nvidia/llama-3.1-nemotron-70b-instruct:free | NVIDIA: Llama 3.1 Nemotron 70B Instruct (free)\n",
      "nvidia/llama-3.1-nemotron-70b-instruct   | NVIDIA: Llama 3.1 Nemotron 70B Instruct\n",
      "inflection/inflection-3-productivity     | Inflection: Inflection 3 Productivity\n",
      "inflection/inflection-3-pi               | Inflection: Inflection 3 Pi\n",
      "google/gemini-flash-1.5-8b               | Google: Gemini 1.5 Flash 8B\n",
      "anthracite-org/magnum-v2-72b             | Magnum v2 72B\n",
      "liquid/lfm-40b                           | Liquid: LFM 40B MoE\n",
      "thedrummer/rocinante-12b                 | Rocinante 12B\n",
      "meta-llama/llama-3.2-1b-instruct:free    | Meta: Llama 3.2 1B Instruct (free)\n",
      "meta-llama/llama-3.2-1b-instruct         | Meta: Llama 3.2 1B Instruct\n",
      "meta-llama/llama-3.2-90b-vision-instruct | Meta: Llama 3.2 90B Vision Instruct\n",
      "meta-llama/llama-3.2-11b-vision-instruct:free | Meta: Llama 3.2 11B Vision Instruct (free)\n",
      "meta-llama/llama-3.2-11b-vision-instruct | Meta: Llama 3.2 11B Vision Instruct\n",
      "meta-llama/llama-3.2-3b-instruct:free    | Meta: Llama 3.2 3B Instruct (free)\n",
      "meta-llama/llama-3.2-3b-instruct         | Meta: Llama 3.2 3B Instruct\n",
      "qwen/qwen-2.5-72b-instruct:free          | Qwen2.5 72B Instruct (free)\n",
      "qwen/qwen-2.5-72b-instruct               | Qwen2.5 72B Instruct\n",
      "qwen/qwen-2.5-vl-72b-instruct            | Qwen: Qwen2.5-VL 72B Instruct\n",
      "neversleep/llama-3.1-lumimaid-8b         | NeverSleep: Lumimaid v0.2 8B\n",
      "openai/o1-mini                           | OpenAI: o1-mini\n",
      "openai/o1-preview                        | OpenAI: o1-preview\n",
      "openai/o1-preview-2024-09-12             | OpenAI: o1-preview (2024-09-12)\n",
      "openai/o1-mini-2024-09-12                | OpenAI: o1-mini (2024-09-12)\n",
      "mistralai/pixtral-12b                    | Mistral: Pixtral 12B\n",
      "cohere/command-r-plus-08-2024            | Cohere: Command R+ (08-2024)\n",
      "cohere/command-r-08-2024                 | Cohere: Command R (08-2024)\n",
      "sao10k/l3.1-euryale-70b                  | Sao10K: Llama 3.1 Euryale 70B v2.2\n",
      "google/gemini-flash-1.5-8b-exp           | Google: Gemini 1.5 Flash 8B Experimental\n",
      "qwen/qwen-2.5-vl-7b-instruct:free        | Qwen: Qwen2.5-VL 7B Instruct (free)\n",
      "qwen/qwen-2.5-vl-7b-instruct             | Qwen: Qwen2.5-VL 7B Instruct\n",
      "ai21/jamba-1-5-mini                      | AI21: Jamba 1.5 Mini\n",
      "ai21/jamba-1-5-large                     | AI21: Jamba 1.5 Large\n",
      "microsoft/phi-3.5-mini-128k-instruct     | Microsoft: Phi-3.5 Mini 128K Instruct\n",
      "nousresearch/hermes-3-llama-3.1-70b      | Nous: Hermes 3 70B Instruct\n",
      "nousresearch/hermes-3-llama-3.1-405b     | Nous: Hermes 3 405B Instruct\n",
      "openai/chatgpt-4o-latest                 | OpenAI: ChatGPT-4o\n",
      "aetherwiing/mn-starcannon-12b            | Aetherwiing: Starcannon 12B\n",
      "sao10k/l3-lunaris-8b                     | Sao10K: Llama 3 8B Lunaris\n",
      "openai/gpt-4o-2024-08-06                 | OpenAI: GPT-4o (2024-08-06)\n",
      "meta-llama/llama-3.1-405b:free           | Meta: Llama 3.1 405B (base) (free)\n",
      "meta-llama/llama-3.1-405b                | Meta: Llama 3.1 405B (base)\n",
      "nothingiisreal/mn-celeste-12b            | Mistral Nemo 12B Celeste\n",
      "perplexity/llama-3.1-sonar-large-128k-online | Perplexity: Llama 3.1 Sonar 70B Online\n",
      "perplexity/llama-3.1-sonar-small-128k-online | Perplexity: Llama 3.1 Sonar 8B Online\n",
      "meta-llama/llama-3.1-8b-instruct:free    | Meta: Llama 3.1 8B Instruct (free)\n",
      "meta-llama/llama-3.1-8b-instruct         | Meta: Llama 3.1 8B Instruct\n",
      "meta-llama/llama-3.1-70b-instruct        | Meta: Llama 3.1 70B Instruct\n",
      "meta-llama/llama-3.1-405b-instruct       | Meta: Llama 3.1 405B Instruct\n",
      "mistralai/codestral-mamba                | Mistral: Codestral Mamba\n",
      "mistralai/mistral-nemo:free              | Mistral: Mistral Nemo (free)\n",
      "mistralai/mistral-nemo                   | Mistral: Mistral Nemo\n",
      "openai/gpt-4o-mini-2024-07-18            | OpenAI: GPT-4o-mini (2024-07-18)\n",
      "openai/gpt-4o-mini                       | OpenAI: GPT-4o-mini\n",
      "google/gemma-2-27b-it                    | Google: Gemma 2 27B\n",
      "alpindale/magnum-72b                     | Magnum 72B\n",
      "google/gemma-2-9b-it:free                | Google: Gemma 2 9B (free)\n",
      "google/gemma-2-9b-it                     | Google: Gemma 2 9B\n",
      "01-ai/yi-large                           | 01.AI: Yi Large\n",
      "ai21/jamba-instruct                      | AI21: Jamba Instruct\n",
      "anthropic/claude-3.5-sonnet-20240620:beta | Anthropic: Claude 3.5 Sonnet (2024-06-20) (self-moderated)\n",
      "anthropic/claude-3.5-sonnet-20240620     | Anthropic: Claude 3.5 Sonnet (2024-06-20)\n",
      "sao10k/l3-euryale-70b                    | Sao10k: Llama 3 Euryale 70B v2.1\n",
      "cognitivecomputations/dolphin-mixtral-8x22b | Dolphin 2.9.2 Mixtral 8x22B 🐬\n",
      "qwen/qwen-2-72b-instruct                 | Qwen 2 72B Instruct\n",
      "nousresearch/hermes-2-pro-llama-3-8b     | NousResearch: Hermes 2 Pro - Llama-3 8B\n",
      "mistralai/mistral-7b-instruct-v0.3       | Mistral: Mistral 7B Instruct v0.3\n",
      "mistralai/mistral-7b-instruct:free       | Mistral: Mistral 7B Instruct (free)\n",
      "mistralai/mistral-7b-instruct            | Mistral: Mistral 7B Instruct\n",
      "microsoft/phi-3-mini-128k-instruct       | Microsoft: Phi-3 Mini 128K Instruct\n",
      "microsoft/phi-3-medium-128k-instruct     | Microsoft: Phi-3 Medium 128K Instruct\n",
      "neversleep/llama-3-lumimaid-70b          | NeverSleep: Llama 3 Lumimaid 70B\n",
      "google/gemini-flash-1.5                  | Google: Gemini 1.5 Flash \n",
      "openai/gpt-4o                            | OpenAI: GPT-4o\n",
      "openai/gpt-4o:extended                   | OpenAI: GPT-4o (extended)\n",
      "openai/gpt-4o-2024-05-13                 | OpenAI: GPT-4o (2024-05-13)\n",
      "meta-llama/llama-guard-2-8b              | Meta: LlamaGuard 2 8B\n",
      "neversleep/llama-3-lumimaid-8b:extended  | NeverSleep: Llama 3 Lumimaid 8B (extended)\n",
      "neversleep/llama-3-lumimaid-8b           | NeverSleep: Llama 3 Lumimaid 8B\n",
      "sao10k/fimbulvetr-11b-v2                 | Fimbulvetr 11B v2\n",
      "meta-llama/llama-3-8b-instruct           | Meta: Llama 3 8B Instruct\n",
      "meta-llama/llama-3-70b-instruct          | Meta: Llama 3 70B Instruct\n",
      "mistralai/mixtral-8x22b-instruct         | Mistral: Mixtral 8x22B Instruct\n",
      "microsoft/wizardlm-2-8x22b               | WizardLM-2 8x22B\n",
      "microsoft/wizardlm-2-7b                  | WizardLM-2 7B\n",
      "google/gemini-pro-1.5                    | Google: Gemini 1.5 Pro\n",
      "openai/gpt-4-turbo                       | OpenAI: GPT-4 Turbo\n",
      "cohere/command-r-plus                    | Cohere: Command R+\n",
      "cohere/command-r-plus-04-2024            | Cohere: Command R+ (04-2024)\n",
      "sophosympatheia/midnight-rose-70b        | Midnight Rose 70B\n",
      "cohere/command-r                         | Cohere: Command R\n",
      "cohere/command                           | Cohere: Command\n",
      "anthropic/claude-3-haiku:beta            | Anthropic: Claude 3 Haiku (self-moderated)\n",
      "anthropic/claude-3-haiku                 | Anthropic: Claude 3 Haiku\n",
      "anthropic/claude-3-sonnet:beta           | Anthropic: Claude 3 Sonnet (self-moderated)\n",
      "anthropic/claude-3-sonnet                | Anthropic: Claude 3 Sonnet\n",
      "anthropic/claude-3-opus:beta             | Anthropic: Claude 3 Opus (self-moderated)\n",
      "anthropic/claude-3-opus                  | Anthropic: Claude 3 Opus\n",
      "cohere/command-r-03-2024                 | Cohere: Command R (03-2024)\n",
      "mistralai/mistral-large                  | Mistral Large\n",
      "openai/gpt-4-turbo-preview               | OpenAI: GPT-4 Turbo Preview\n",
      "openai/gpt-3.5-turbo-0613                | OpenAI: GPT-3.5 Turbo (older v0613)\n",
      "nousresearch/nous-hermes-2-mixtral-8x7b-dpo | Nous: Hermes 2 Mixtral 8x7B DPO\n",
      "mistralai/mistral-small                  | Mistral Small\n",
      "mistralai/mistral-medium                 | Mistral Medium\n",
      "mistralai/mistral-tiny                   | Mistral Tiny\n",
      "mistralai/mistral-7b-instruct-v0.2       | Mistral: Mistral 7B Instruct v0.2\n",
      "cognitivecomputations/dolphin-mixtral-8x7b | Dolphin 2.6 Mixtral 8x7B 🐬\n",
      "google/gemini-pro                        | Google: Gemini Pro 1.0\n",
      "google/gemini-pro-vision                 | Google: Gemini Pro Vision 1.0\n",
      "mistralai/mixtral-8x7b-instruct          | Mistral: Mixtral 8x7B Instruct\n",
      "openchat/openchat-7b                     | OpenChat 3.5 7B\n",
      "neversleep/noromaid-20b                  | Noromaid 20B\n",
      "anthropic/claude-2:beta                  | Anthropic: Claude v2 (self-moderated)\n",
      "anthropic/claude-2                       | Anthropic: Claude v2\n",
      "anthropic/claude-2.1:beta                | Anthropic: Claude v2.1 (self-moderated)\n",
      "anthropic/claude-2.1                     | Anthropic: Claude v2.1\n",
      "alpindale/goliath-120b                   | Goliath 120B\n",
      "undi95/toppy-m-7b                        | Toppy M 7B\n",
      "openrouter/auto                          | Auto Router\n",
      "openai/gpt-4-1106-preview                | OpenAI: GPT-4 Turbo (older v1106)\n",
      "openai/gpt-3.5-turbo-1106                | OpenAI: GPT-3.5 Turbo 16k (older v1106)\n",
      "google/palm-2-chat-bison-32k             | Google: PaLM 2 Chat 32k\n",
      "google/palm-2-codechat-bison-32k         | Google: PaLM 2 Code Chat 32k\n",
      "jondurbin/airoboros-l2-70b               | Airoboros 70B\n",
      "mistralai/mistral-7b-instruct-v0.1       | Mistral: Mistral 7B Instruct v0.1\n",
      "openai/gpt-3.5-turbo-instruct            | OpenAI: GPT-3.5 Turbo Instruct\n",
      "pygmalionai/mythalion-13b                | Pygmalion: Mythalion 13B\n",
      "openai/gpt-4-32k                         | OpenAI: GPT-4 32k\n",
      "openai/gpt-3.5-turbo-16k                 | OpenAI: GPT-3.5 Turbo 16k\n",
      "openai/gpt-4-32k-0314                    | OpenAI: GPT-4 32k (older v0314)\n",
      "nousresearch/nous-hermes-llama2-13b      | Nous: Hermes 13B\n",
      "mancer/weaver                            | Mancer: Weaver (alpha)\n",
      "huggingfaceh4/zephyr-7b-beta:free        | Hugging Face: Zephyr 7B (free)\n",
      "anthropic/claude-2.0:beta                | Anthropic: Claude v2.0 (self-moderated)\n",
      "anthropic/claude-2.0                     | Anthropic: Claude v2.0\n",
      "undi95/remm-slerp-l2-13b                 | ReMM SLERP 13B\n",
      "google/palm-2-codechat-bison             | Google: PaLM 2 Code Chat\n",
      "google/palm-2-chat-bison                 | Google: PaLM 2 Chat\n",
      "gryphe/mythomax-l2-13b                   | MythoMax 13B\n",
      "meta-llama/llama-2-13b-chat              | Meta: Llama 2 13B Chat\n",
      "meta-llama/llama-2-70b-chat              | Meta: Llama 2 70B Chat\n",
      "openai/gpt-3.5-turbo                     | OpenAI: GPT-3.5 Turbo\n",
      "openai/gpt-4                             | OpenAI: GPT-4\n",
      "openai/gpt-3.5-turbo-0125                | OpenAI: GPT-3.5 Turbo 16k\n",
      "openai/gpt-4-0314                        | OpenAI: GPT-4 (older v0314)\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import requests\n",
    "\n",
    "# Set your API Key and Base URL\n",
    "OPENROUTER_API_KEY = \"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\"\n",
    "openai.api_key = OPENROUTER_API_KEY\n",
    "openai.api_base = \"https://openrouter.ai/api/v1\"\n",
    "\n",
    "# Function to list available models\n",
    "def list_openrouter_models():\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\"\n",
    "    }\n",
    "    response = requests.get(\"https://openrouter.ai/api/v1/models\", headers=headers)\n",
    "\n",
    "    print(\"📦 Raw response (as text):\\n\", response.text[:500])  # show only the first 500 chars for sanity\n",
    "\n",
    "    try:\n",
    "        full_json = response.json()\n",
    "        models = full_json.get(\"data\", [])\n",
    "\n",
    "        print(\"✅ Available Models on OpenRouter:\\n\")\n",
    "        for model in models:\n",
    "            print(f\"{model['id']:40} | {model['name']}\")\n",
    "    except Exception as e:\n",
    "        print(\"❌ Failed to parse JSON:\", e)\n",
    "\n",
    "    \n",
    "# Function to run a prompt\n",
    "def run_openrouter_prompt(prompt_text, model=\"mistralai/mixtral-8x7b-instruct\"):\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert in financial analysis and KPI extraction.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt_text}\n",
    "            ]\n",
    "        )\n",
    "        print(\"🧠 Response:\")\n",
    "        print(response['choices'][0]['message']['content'])\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")\n",
    "\n",
    "# Step 1: List models\n",
    "list_openrouter_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bc8f21-9637-4ecc-8514-e4dbf0921b64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
